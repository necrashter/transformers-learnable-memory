{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25af8812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 17:39:59.708897: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-03 17:40:00.262934: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory\n",
      "2023-05-03 17:40:00.263081: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory\n",
      "2023-05-03 17:40:00.263088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Some weights of the model checkpoint at google/vit-base-patch32-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch32-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTModel, ViTConfig, ViTForImageClassification\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "cache_dir = \"/home/egeberk/ceng502/\"\n",
    "num_classes = 10\n",
    "model_name = 'google/vit-base-patch32-224-in21k'\n",
    "\n",
    "config = ViTConfig.from_pretrained(model_name, num_labels=num_classes, cache_dir=cache_dir)\n",
    "model = ViTForImageClassification.from_pretrained(model_name, config=config, cache_dir=cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b14a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c180874",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4ca498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the head\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346e12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd09e88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-4\n",
    "batch_size = 124\n",
    "num_epochs = 10 \n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set the optimizer and loss function\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb98a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(modelVit):\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = modelVit(data)\n",
    "            try:\n",
    "                loss = criterion(outputs.logits, targets)\n",
    "            except:\n",
    "                loss = criterion(outputs, targets)\n",
    "            loss_val = loss.detach().cpu().item()\n",
    "            train_loss += loss_val\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient descent step\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader)}\")\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b2b765f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15753a5168104e90b6b8a3830800b17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0738051921719372\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b54d9103ef495ca9b592c557f1b15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.43248644522806207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d56ecf3addc47e2803f379e7a5d8f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.33499665012454044\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078a896dd5b34eee9e124a9edf9ce8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.2948971911158302\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81002d3323d445d297f1b27b14498a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.2710435390287992\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8f8e2bb2de4676a4a255153fdc69b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.2558640625586014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e662fa87e6475fac819242894ee45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.24442257984808766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7515922843534ee39336eee600f5c8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.23512031082617174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922df80fc70d435082562e74c0e579d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.22762656370454495\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb14d8e99cf458fa777d23389b9faa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.22125080657551194\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d783f4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eaa88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(modelVit):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(tqdm(valid_loader)):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = modelVit(data)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            try:\n",
    "                _, predicted = torch.max(outputs.logits, 1)\n",
    "            except:\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "104b83c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c1a9d8f5ec46c98fe197a187463cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 92 %\n"
     ]
    }
   ],
   "source": [
    "valid(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a96e5a",
   "metadata": {},
   "source": [
    "# Class + head fine tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3489e6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.embeddings.cls_token\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb43e58cc6f74b698e3dbd86a72a00b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0607944261467104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3efd5e008d4eda8011d0bf03310a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.3625623905053823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4685226cb246a9a27094ab3a4e761e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.2548982919664076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0344fb98d8ef403f8a12246d5db6f40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.2127182101185369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c8018c095346269ba7594f576cc006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.1896207969598841\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59624c03301d4c3ab7e8cf7b2807d1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.17512742105391946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25ca38706e24aac9025a0e7bdbbeb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.1644193258800424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346ae3741bae4b1a95a6810bc486012b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.1563596963181649\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8986f79449b049eea3c1be98efb40217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.14986024178232593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c7ee61f04147caba4e638497be9f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.14483140294652175\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "# Freeze all layers except the head\n",
    "for name, param in model.named_parameters():\n",
    "    if (\"classifier\" not in name) and (\"cls_token\" not in name) :\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        print(name)\n",
    "        \n",
    "        \n",
    "# Load and preprocess the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "learning_rate = 3e-4\n",
    "batch_size = 124\n",
    "num_epochs = 10 \n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set the optimizer and loss function\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407bd0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad28861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7ab03c1a334b83bf1e9229ad189584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 94 %\n"
     ]
    }
   ],
   "source": [
    "valid(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c02776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1826e977",
   "metadata": {},
   "source": [
    "# Memory Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f3fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Set, Tuple, Union\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3c3d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomViTLayerForMemory(nn.Module):\n",
    "    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig, attention, intermediate, output, layernorm_b, layernorm_a) -> None:\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = attention #ViTAttention(config)\n",
    "        self.intermediate = intermediate #ViTIntermediate(config)\n",
    "        self.output = output #ViTOutput(config)\n",
    "        self.layernorm_before = layernorm_b #nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after = layernorm_a #nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        memory_token: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        \n",
    "        original_states = hidden_states\n",
    "        \n",
    "        if memory_token is not None:\n",
    "            hidden_states = torch.cat((hidden_states, memory_token.expand(hidden_states.shape[0], -1, -1)), dim=1)\n",
    "        \n",
    "        self_attention_outputs = self.attention(\n",
    "            self.layernorm_before(hidden_states),  # in ViT, layernorm is applied before self-attention\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        \n",
    "        if memory_token is not None:\n",
    "            attention_output = self_attention_outputs[0][:,:-memory_token.shape[0]] #only input attends\n",
    "        else:\n",
    "            attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        # first residual connection\n",
    "        hidden_states = attention_output + original_states\n",
    "\n",
    "        # in ViT, layernorm is also applied after self-attention\n",
    "        layer_output = self.layernorm_after(hidden_states)\n",
    "        layer_output = self.intermediate(layer_output)\n",
    "\n",
    "        # second residual connection is done here\n",
    "        layer_output = self.output(layer_output, hidden_states)\n",
    "\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67f924b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperModel(nn.Module):\n",
    "    def __init__(self, config, vit, embed_dim = 768, memory_token_length = 10):\n",
    "        super(PaperModel, self).__init__()\n",
    "        self.memory_token_length = memory_token_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.model = vit\n",
    "        \n",
    "        for index, layer_ in enumerate(self.model.vit.encoder.layer):\n",
    "            self.model.vit.encoder.layer[index] = CustomViTLayerForMemory(config, layer_.attention, layer_.intermediate, layer_.output, layer_.layernorm_before, layer_.layernorm_after)\n",
    "        \n",
    "        self.memory_token = torch.nn.Parameter(\n",
    "            torch.randn(self.memory_token_length, self.embed_dim)\n",
    "        ) #memory token\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden_states = self.model.vit.embeddings(x)\n",
    "        \n",
    "        for layer in self.model.vit.encoder.layer:\n",
    "            hidden_states = layer(hidden_states, self.memory_token)[0]            \n",
    "        \n",
    "        normalized = self.model.vit.layernorm(hidden_states)\n",
    "        logits = self.model.classifier(normalized[:, 0, :])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccc78cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = PaperModel(config, model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "929eccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to reset grads\n",
    "for name, param in pm.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d22e043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.vit.embeddings.cls_token\n",
      "model.classifier.weight\n",
      "model.classifier.bias\n"
     ]
    }
   ],
   "source": [
    "#memory + cls + head\n",
    "for name, param in pm.named_parameters():\n",
    "    if (\"classifier\" not in name) and (\"cls_token\" not in name) and (\"memory_token\" not in name):\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8bfb535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d79bff24b8486dbc9ed30b0209946d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.0713421274529824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59aca18d52e45efb570ec63e2365ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 1.5238722368042068\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f551c20dc5f4692bba9c2e2105f3526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 1.1159659908844692\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8782a14ab859418db1c0ff4b3b7bf587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.8639323146331428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e41aa259b2458690d2490f816331de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.7165755905432276\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36c85a8c8e248b68cb7182f52281628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.6236999492243965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b13d793da94a79906a838cda62fcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.5577812611614124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843f5682932746a2a5cccd4f5d13a6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.5099749285543319\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2619f04fe09e460699042dfc8496f630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.4726340124512663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79fe7b16682421cb0a4cd21fb637a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.4428345797085526\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84be9fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05961cb4bb9d4edf9add34696ed22369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 87 %\n"
     ]
    }
   ],
   "source": [
    "valid(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = PaperModel(config, model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90193f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccecf077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48add188a0c4773867d19af0b4e83e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.27229671062219263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bc2ff5719f47198b022556710ed2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474ee75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82395520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1caa813e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a731ad5ccf824cb695f0976056750e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_idx, (data, targets) in enumerate(tqdm(valid_loader)):\n",
    "    data = data.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e9f942e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0278, -0.8870,  1.5921,  ..., -1.4284, -0.4906, -0.6489],\n",
       "        [ 0.3318, -0.6451,  1.4786,  ..., -1.2412,  0.2162, -0.3718],\n",
       "        [ 0.2759, -0.6854,  1.3918,  ..., -1.2371,  0.3058, -0.3987],\n",
       "        ...,\n",
       "        [-0.0230, -0.9826,  1.5481,  ..., -1.1421, -0.3711, -0.2848],\n",
       "        [-0.1069, -0.4906,  1.4808,  ..., -1.5035, -0.3257, -0.0312],\n",
       "        [ 0.1012, -1.0433,  2.1145,  ..., -1.3779, -0.4260, -0.6846]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd316029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customAttentionWithMemory(nn.Module):\n",
    "    def __init__(self, config: ViTConfig, query, key, value, dropout) -> None:\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n",
    "                f\"heads {config.num_attention_heads}.\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = query\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self, q, k, v):\n",
    "        mixed_query_layer = self.query(q)\n",
    "\n",
    "        key_layer = self.transpose_for_scores(self.key(k))\n",
    "        value_layer = self.transpose_for_scores(self.value(v))\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c759bb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 17 (1751082145.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[72], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    att_out = self.layer(x, self.number_of_head)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 17\n"
     ]
    }
   ],
   "source": [
    "class PaperModel(nn.Module):\n",
    "    def __init__(self, vit, embed_dim = 768, memory_token_length = 10):\n",
    "        super(PaperModel, self).__init__()\n",
    "        self.memory_token_length = memory_token_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.model = vit\n",
    "        self.memory_token = torch.nn.Parameter(\n",
    "            torch.randn(self.memory_token_length, self.embed_dim)\n",
    "        ) #memory token\n",
    "        for layer in self.model.vit.encoder:\n",
    "            att_model = layer.attention\n",
    "            layer.attention = customAttentionWithMemory(config, att_model.attention.query,\\\n",
    "                                                        att_model.attention.key, att_model.attention.value,att_model.attention.dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        input = self.model.vit.embeddings(x)\n",
    "        for layer in self.model.vit.encoder.layer:\n",
    "            \n",
    "        \n",
    "        att_out = self.layer(x, self.number_of_head)\n",
    "        att_residual_out = att_out + x\n",
    "        norm1_out = self.dropout1(self.norm1(att_residual_out))\n",
    "        ff_out = self.MLP_sequence(norm1_out)\n",
    "        ff_res_out = ff_out + norm1_out\n",
    "        norm2_out = self.dropout2(self.norm2(ff_res_out))\n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be85f367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecec121b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.1750,  1.2434,  0.0246,  ...,  1.0854,  0.5624,  1.8326],\n",
       "          [ 0.5839,  0.4748, -0.9245,  ...,  0.4982,  0.0449,  1.3741],\n",
       "          [-0.5252,  1.6358, -0.8340,  ...,  0.5063,  0.6809,  1.2358],\n",
       "          ...,\n",
       "          [ 1.5790,  1.5951, -0.5681,  ..., -0.0584,  0.5637,  0.7831],\n",
       "          [ 0.4778,  0.8916, -0.3458,  ...,  0.7740,  0.7255,  1.5669],\n",
       "          [-0.1720,  2.2242,  0.1825,  ...,  0.7067,  0.1703,  1.1183]],\n",
       " \n",
       "         [[ 0.0317,  1.9712, -1.1072,  ...,  1.0647,  0.2739,  1.5160],\n",
       "          [-0.5021,  2.0634, -1.3776,  ...,  0.7768,  0.0027,  1.1941],\n",
       "          [ 0.5766,  0.7833, -0.5032,  ...,  0.6730, -0.1111,  0.7818],\n",
       "          ...,\n",
       "          [ 0.6456,  1.2389, -1.0500,  ...,  0.8230, -0.2533,  1.4723],\n",
       "          [-0.2427,  0.3021, -1.5380,  ...,  0.9418, -0.4302,  0.5285],\n",
       "          [ 0.6844,  1.5814, -0.3445,  ...,  0.5765, -0.6269,  1.1135]],\n",
       " \n",
       "         [[ 0.0652,  0.3140, -0.6031,  ...,  0.4982,  1.1300,  2.2200],\n",
       "          [ 0.7496,  1.8503, -0.3597,  ...,  0.4807,  0.8226,  0.8485],\n",
       "          [ 0.1290,  1.8189, -1.1795,  ...,  0.4288,  0.5787,  1.2097],\n",
       "          ...,\n",
       "          [ 0.2937,  1.5051, -1.1848,  ...,  0.6211,  0.4715,  1.1732],\n",
       "          [ 0.1129,  2.3061, -0.2484,  ...,  0.4988, -0.0814,  1.7406],\n",
       "          [ 0.0974,  1.3779, -0.8254,  ...,  0.2124, -0.5762,  0.5515]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.3216,  1.5224, -0.9534,  ...,  1.2367,  0.7969,  1.2935],\n",
       "          [ 0.3551,  0.8337, -1.0850,  ...,  0.1682,  0.4069, -0.1397],\n",
       "          [ 0.6770,  1.2924, -0.6037,  ...,  0.6356, -0.4211,  0.3495],\n",
       "          ...,\n",
       "          [-0.4930,  0.9827, -0.5448,  ...,  0.8914,  0.0974,  1.2046],\n",
       "          [ 0.3798,  1.8572, -0.8579,  ...,  0.3174,  0.6701,  1.5018],\n",
       "          [ 1.0607,  0.5246, -0.2792,  ...,  0.7054,  0.7901, -0.2080]],\n",
       " \n",
       "         [[ 0.0965,  1.5849,  0.0803,  ...,  0.6281, -0.7096,  1.4393],\n",
       "          [ 0.7310,  1.5263, -1.1805,  ...,  1.0038, -0.0543, -0.4456],\n",
       "          [ 0.2883,  1.0404, -1.1583,  ...,  0.1801,  1.2232,  1.7764],\n",
       "          ...,\n",
       "          [ 0.2465,  1.4299, -0.8948,  ...,  0.7821,  0.5200,  1.3264],\n",
       "          [-0.2761,  0.8486, -0.6597,  ...,  0.6752,  0.2748,  2.2627],\n",
       "          [-0.2453,  1.4308, -0.7784,  ...,  0.4823,  0.7658,  1.5281]],\n",
       " \n",
       "         [[ 0.4167,  0.9356, -1.3095,  ...,  0.3280,  0.1157,  0.6103],\n",
       "          [ 0.9634,  1.3174, -0.1172,  ...,  0.9718, -0.1917,  1.8015],\n",
       "          [ 0.0054,  1.0056, -0.6831,  ...,  0.5208,  0.6431,  1.2557],\n",
       "          ...,\n",
       "          [ 0.4376,  2.2553, -0.7913,  ...,  0.8399,  0.5072,  0.5553],\n",
       "          [ 0.6856,  0.7474,  0.1166,  ...,  0.5667,  0.3252,  1.4412],\n",
       "          [ 0.4203,  0.4814, -0.2715,  ...,  0.7466, -0.2142,  1.4803]]],\n",
       "        device='cuda:0'),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vit.encoder.layer[0](k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a23e1d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand([28, 50, 768])\n",
    "expanded_memory = torch.randn((10, 768))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fa5ab42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 10, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_memory.expand(28, -1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09ac6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.cat((q, expanded_memory.expand(28, -1, -1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8b6f029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 60, 768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab19bad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 50, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:,:-expanded_memory.shape[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af9dab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(s.shape).to(device)\n",
    "k = torch.rand([28, 60, 768]).to(device)\n",
    "v = torch.rand([28, 60, 768]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fecb7cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a = model.vit.encoder.layer[0].attention.attention.query(q)\n",
    "k_a = model.vit.encoder.layer[0].attention.attention.key(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a64c27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = q.matmul(k.transpose(-1, -2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1f74cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTLayer(\n",
       "  (attention): ViTAttention(\n",
       "    (attention): ViTSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (output): ViTSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): ViTIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): ViTOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_ = model.vit.encoder.layer[0]\n",
    "layer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "710a4ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_layer_ = CustomViTLayerForMemory(config, layer_.attention, layer_.intermediate, layer_.output, layer_.layernorm_before, layer_.layernorm_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4cb05437",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_layer_ = custom_layer_.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "576da9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q.to(device)\n",
    "expanded_memory = expanded_memory.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a1d8f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 50, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_layer_(q, expanded_memory)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "74998cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a15f5d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 50, 60])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f9a270bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_model = model.vit.encoder.layer[0].attention\n",
    "\n",
    "ca = customAttentionWithMemory(config, att_model.attention.query, att_model.attention.key, att_model.attention.value, att_model.attention.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e5d8d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 60, 768])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "725bf7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 50, 768])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca(q,k,v)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b35ad80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vit.config.attention_probs_dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9bb433a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTLayer(\n",
       "  (attention): ViTAttention(\n",
       "    (attention): ViTSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (output): ViTSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): ViTIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): ViTOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vit.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430fffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a41336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a710ae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.embeddings.cls_token\n",
      "vit.embeddings.position_embeddings\n",
      "vit.embeddings.patch_embeddings.projection.weight\n",
      "vit.embeddings.patch_embeddings.projection.bias\n",
      "vit.encoder.layer.0.attention.attention.query.weight\n",
      "vit.encoder.layer.0.attention.attention.query.bias\n",
      "vit.encoder.layer.0.attention.attention.key.weight\n",
      "vit.encoder.layer.0.attention.attention.key.bias\n",
      "vit.encoder.layer.0.attention.attention.value.weight\n",
      "vit.encoder.layer.0.attention.attention.value.bias\n",
      "vit.encoder.layer.0.attention.output.dense.weight\n",
      "vit.encoder.layer.0.attention.output.dense.bias\n",
      "vit.encoder.layer.0.intermediate.dense.weight\n",
      "vit.encoder.layer.0.intermediate.dense.bias\n",
      "vit.encoder.layer.0.output.dense.weight\n",
      "vit.encoder.layer.0.output.dense.bias\n",
      "vit.encoder.layer.0.layernorm_before.weight\n",
      "vit.encoder.layer.0.layernorm_before.bias\n",
      "vit.encoder.layer.0.layernorm_after.weight\n",
      "vit.encoder.layer.0.layernorm_after.bias\n",
      "vit.encoder.layer.1.attention.attention.query.weight\n",
      "vit.encoder.layer.1.attention.attention.query.bias\n",
      "vit.encoder.layer.1.attention.attention.key.weight\n",
      "vit.encoder.layer.1.attention.attention.key.bias\n",
      "vit.encoder.layer.1.attention.attention.value.weight\n",
      "vit.encoder.layer.1.attention.attention.value.bias\n",
      "vit.encoder.layer.1.attention.output.dense.weight\n",
      "vit.encoder.layer.1.attention.output.dense.bias\n",
      "vit.encoder.layer.1.intermediate.dense.weight\n",
      "vit.encoder.layer.1.intermediate.dense.bias\n",
      "vit.encoder.layer.1.output.dense.weight\n",
      "vit.encoder.layer.1.output.dense.bias\n",
      "vit.encoder.layer.1.layernorm_before.weight\n",
      "vit.encoder.layer.1.layernorm_before.bias\n",
      "vit.encoder.layer.1.layernorm_after.weight\n",
      "vit.encoder.layer.1.layernorm_after.bias\n",
      "vit.encoder.layer.2.attention.attention.query.weight\n",
      "vit.encoder.layer.2.attention.attention.query.bias\n",
      "vit.encoder.layer.2.attention.attention.key.weight\n",
      "vit.encoder.layer.2.attention.attention.key.bias\n",
      "vit.encoder.layer.2.attention.attention.value.weight\n",
      "vit.encoder.layer.2.attention.attention.value.bias\n",
      "vit.encoder.layer.2.attention.output.dense.weight\n",
      "vit.encoder.layer.2.attention.output.dense.bias\n",
      "vit.encoder.layer.2.intermediate.dense.weight\n",
      "vit.encoder.layer.2.intermediate.dense.bias\n",
      "vit.encoder.layer.2.output.dense.weight\n",
      "vit.encoder.layer.2.output.dense.bias\n",
      "vit.encoder.layer.2.layernorm_before.weight\n",
      "vit.encoder.layer.2.layernorm_before.bias\n",
      "vit.encoder.layer.2.layernorm_after.weight\n",
      "vit.encoder.layer.2.layernorm_after.bias\n",
      "vit.encoder.layer.3.attention.attention.query.weight\n",
      "vit.encoder.layer.3.attention.attention.query.bias\n",
      "vit.encoder.layer.3.attention.attention.key.weight\n",
      "vit.encoder.layer.3.attention.attention.key.bias\n",
      "vit.encoder.layer.3.attention.attention.value.weight\n",
      "vit.encoder.layer.3.attention.attention.value.bias\n",
      "vit.encoder.layer.3.attention.output.dense.weight\n",
      "vit.encoder.layer.3.attention.output.dense.bias\n",
      "vit.encoder.layer.3.intermediate.dense.weight\n",
      "vit.encoder.layer.3.intermediate.dense.bias\n",
      "vit.encoder.layer.3.output.dense.weight\n",
      "vit.encoder.layer.3.output.dense.bias\n",
      "vit.encoder.layer.3.layernorm_before.weight\n",
      "vit.encoder.layer.3.layernorm_before.bias\n",
      "vit.encoder.layer.3.layernorm_after.weight\n",
      "vit.encoder.layer.3.layernorm_after.bias\n",
      "vit.encoder.layer.4.attention.attention.query.weight\n",
      "vit.encoder.layer.4.attention.attention.query.bias\n",
      "vit.encoder.layer.4.attention.attention.key.weight\n",
      "vit.encoder.layer.4.attention.attention.key.bias\n",
      "vit.encoder.layer.4.attention.attention.value.weight\n",
      "vit.encoder.layer.4.attention.attention.value.bias\n",
      "vit.encoder.layer.4.attention.output.dense.weight\n",
      "vit.encoder.layer.4.attention.output.dense.bias\n",
      "vit.encoder.layer.4.intermediate.dense.weight\n",
      "vit.encoder.layer.4.intermediate.dense.bias\n",
      "vit.encoder.layer.4.output.dense.weight\n",
      "vit.encoder.layer.4.output.dense.bias\n",
      "vit.encoder.layer.4.layernorm_before.weight\n",
      "vit.encoder.layer.4.layernorm_before.bias\n",
      "vit.encoder.layer.4.layernorm_after.weight\n",
      "vit.encoder.layer.4.layernorm_after.bias\n",
      "vit.encoder.layer.5.attention.attention.query.weight\n",
      "vit.encoder.layer.5.attention.attention.query.bias\n",
      "vit.encoder.layer.5.attention.attention.key.weight\n",
      "vit.encoder.layer.5.attention.attention.key.bias\n",
      "vit.encoder.layer.5.attention.attention.value.weight\n",
      "vit.encoder.layer.5.attention.attention.value.bias\n",
      "vit.encoder.layer.5.attention.output.dense.weight\n",
      "vit.encoder.layer.5.attention.output.dense.bias\n",
      "vit.encoder.layer.5.intermediate.dense.weight\n",
      "vit.encoder.layer.5.intermediate.dense.bias\n",
      "vit.encoder.layer.5.output.dense.weight\n",
      "vit.encoder.layer.5.output.dense.bias\n",
      "vit.encoder.layer.5.layernorm_before.weight\n",
      "vit.encoder.layer.5.layernorm_before.bias\n",
      "vit.encoder.layer.5.layernorm_after.weight\n",
      "vit.encoder.layer.5.layernorm_after.bias\n",
      "vit.encoder.layer.6.attention.attention.query.weight\n",
      "vit.encoder.layer.6.attention.attention.query.bias\n",
      "vit.encoder.layer.6.attention.attention.key.weight\n",
      "vit.encoder.layer.6.attention.attention.key.bias\n",
      "vit.encoder.layer.6.attention.attention.value.weight\n",
      "vit.encoder.layer.6.attention.attention.value.bias\n",
      "vit.encoder.layer.6.attention.output.dense.weight\n",
      "vit.encoder.layer.6.attention.output.dense.bias\n",
      "vit.encoder.layer.6.intermediate.dense.weight\n",
      "vit.encoder.layer.6.intermediate.dense.bias\n",
      "vit.encoder.layer.6.output.dense.weight\n",
      "vit.encoder.layer.6.output.dense.bias\n",
      "vit.encoder.layer.6.layernorm_before.weight\n",
      "vit.encoder.layer.6.layernorm_before.bias\n",
      "vit.encoder.layer.6.layernorm_after.weight\n",
      "vit.encoder.layer.6.layernorm_after.bias\n",
      "vit.encoder.layer.7.attention.attention.query.weight\n",
      "vit.encoder.layer.7.attention.attention.query.bias\n",
      "vit.encoder.layer.7.attention.attention.key.weight\n",
      "vit.encoder.layer.7.attention.attention.key.bias\n",
      "vit.encoder.layer.7.attention.attention.value.weight\n",
      "vit.encoder.layer.7.attention.attention.value.bias\n",
      "vit.encoder.layer.7.attention.output.dense.weight\n",
      "vit.encoder.layer.7.attention.output.dense.bias\n",
      "vit.encoder.layer.7.intermediate.dense.weight\n",
      "vit.encoder.layer.7.intermediate.dense.bias\n",
      "vit.encoder.layer.7.output.dense.weight\n",
      "vit.encoder.layer.7.output.dense.bias\n",
      "vit.encoder.layer.7.layernorm_before.weight\n",
      "vit.encoder.layer.7.layernorm_before.bias\n",
      "vit.encoder.layer.7.layernorm_after.weight\n",
      "vit.encoder.layer.7.layernorm_after.bias\n",
      "vit.encoder.layer.8.attention.attention.query.weight\n",
      "vit.encoder.layer.8.attention.attention.query.bias\n",
      "vit.encoder.layer.8.attention.attention.key.weight\n",
      "vit.encoder.layer.8.attention.attention.key.bias\n",
      "vit.encoder.layer.8.attention.attention.value.weight\n",
      "vit.encoder.layer.8.attention.attention.value.bias\n",
      "vit.encoder.layer.8.attention.output.dense.weight\n",
      "vit.encoder.layer.8.attention.output.dense.bias\n",
      "vit.encoder.layer.8.intermediate.dense.weight\n",
      "vit.encoder.layer.8.intermediate.dense.bias\n",
      "vit.encoder.layer.8.output.dense.weight\n",
      "vit.encoder.layer.8.output.dense.bias\n",
      "vit.encoder.layer.8.layernorm_before.weight\n",
      "vit.encoder.layer.8.layernorm_before.bias\n",
      "vit.encoder.layer.8.layernorm_after.weight\n",
      "vit.encoder.layer.8.layernorm_after.bias\n",
      "vit.encoder.layer.9.attention.attention.query.weight\n",
      "vit.encoder.layer.9.attention.attention.query.bias\n",
      "vit.encoder.layer.9.attention.attention.key.weight\n",
      "vit.encoder.layer.9.attention.attention.key.bias\n",
      "vit.encoder.layer.9.attention.attention.value.weight\n",
      "vit.encoder.layer.9.attention.attention.value.bias\n",
      "vit.encoder.layer.9.attention.output.dense.weight\n",
      "vit.encoder.layer.9.attention.output.dense.bias\n",
      "vit.encoder.layer.9.intermediate.dense.weight\n",
      "vit.encoder.layer.9.intermediate.dense.bias\n",
      "vit.encoder.layer.9.output.dense.weight\n",
      "vit.encoder.layer.9.output.dense.bias\n",
      "vit.encoder.layer.9.layernorm_before.weight\n",
      "vit.encoder.layer.9.layernorm_before.bias\n",
      "vit.encoder.layer.9.layernorm_after.weight\n",
      "vit.encoder.layer.9.layernorm_after.bias\n",
      "vit.encoder.layer.10.attention.attention.query.weight\n",
      "vit.encoder.layer.10.attention.attention.query.bias\n",
      "vit.encoder.layer.10.attention.attention.key.weight\n",
      "vit.encoder.layer.10.attention.attention.key.bias\n",
      "vit.encoder.layer.10.attention.attention.value.weight\n",
      "vit.encoder.layer.10.attention.attention.value.bias\n",
      "vit.encoder.layer.10.attention.output.dense.weight\n",
      "vit.encoder.layer.10.attention.output.dense.bias\n",
      "vit.encoder.layer.10.intermediate.dense.weight\n",
      "vit.encoder.layer.10.intermediate.dense.bias\n",
      "vit.encoder.layer.10.output.dense.weight\n",
      "vit.encoder.layer.10.output.dense.bias\n",
      "vit.encoder.layer.10.layernorm_before.weight\n",
      "vit.encoder.layer.10.layernorm_before.bias\n",
      "vit.encoder.layer.10.layernorm_after.weight\n",
      "vit.encoder.layer.10.layernorm_after.bias\n",
      "vit.encoder.layer.11.attention.attention.query.weight\n",
      "vit.encoder.layer.11.attention.attention.query.bias\n",
      "vit.encoder.layer.11.attention.attention.key.weight\n",
      "vit.encoder.layer.11.attention.attention.key.bias\n",
      "vit.encoder.layer.11.attention.attention.value.weight\n",
      "vit.encoder.layer.11.attention.attention.value.bias\n",
      "vit.encoder.layer.11.attention.output.dense.weight\n",
      "vit.encoder.layer.11.attention.output.dense.bias\n",
      "vit.encoder.layer.11.intermediate.dense.weight\n",
      "vit.encoder.layer.11.intermediate.dense.bias\n",
      "vit.encoder.layer.11.output.dense.weight\n",
      "vit.encoder.layer.11.output.dense.bias\n",
      "vit.encoder.layer.11.layernorm_before.weight\n",
      "vit.encoder.layer.11.layernorm_before.bias\n",
      "vit.encoder.layer.11.layernorm_after.weight\n",
      "vit.encoder.layer.11.layernorm_after.bias\n",
      "vit.layernorm.weight\n",
      "vit.layernorm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for i,p in model.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f223cf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.cls_token\n",
      "embeddings.position_embeddings\n",
      "embeddings.patch_embeddings.projection.weight\n",
      "embeddings.patch_embeddings.projection.bias\n",
      "encoder.layer.0.attention.attention.query.weight\n",
      "encoder.layer.0.attention.attention.query.bias\n",
      "encoder.layer.0.attention.attention.key.weight\n",
      "encoder.layer.0.attention.attention.key.bias\n",
      "encoder.layer.0.attention.attention.value.weight\n",
      "encoder.layer.0.attention.attention.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.layernorm_before.weight\n",
      "encoder.layer.0.layernorm_before.bias\n",
      "encoder.layer.0.layernorm_after.weight\n",
      "encoder.layer.0.layernorm_after.bias\n",
      "encoder.layer.1.attention.attention.query.weight\n",
      "encoder.layer.1.attention.attention.query.bias\n",
      "encoder.layer.1.attention.attention.key.weight\n",
      "encoder.layer.1.attention.attention.key.bias\n",
      "encoder.layer.1.attention.attention.value.weight\n",
      "encoder.layer.1.attention.attention.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.layernorm_before.weight\n",
      "encoder.layer.1.layernorm_before.bias\n",
      "encoder.layer.1.layernorm_after.weight\n",
      "encoder.layer.1.layernorm_after.bias\n",
      "encoder.layer.2.attention.attention.query.weight\n",
      "encoder.layer.2.attention.attention.query.bias\n",
      "encoder.layer.2.attention.attention.key.weight\n",
      "encoder.layer.2.attention.attention.key.bias\n",
      "encoder.layer.2.attention.attention.value.weight\n",
      "encoder.layer.2.attention.attention.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.layernorm_before.weight\n",
      "encoder.layer.2.layernorm_before.bias\n",
      "encoder.layer.2.layernorm_after.weight\n",
      "encoder.layer.2.layernorm_after.bias\n",
      "encoder.layer.3.attention.attention.query.weight\n",
      "encoder.layer.3.attention.attention.query.bias\n",
      "encoder.layer.3.attention.attention.key.weight\n",
      "encoder.layer.3.attention.attention.key.bias\n",
      "encoder.layer.3.attention.attention.value.weight\n",
      "encoder.layer.3.attention.attention.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.layernorm_before.weight\n",
      "encoder.layer.3.layernorm_before.bias\n",
      "encoder.layer.3.layernorm_after.weight\n",
      "encoder.layer.3.layernorm_after.bias\n",
      "encoder.layer.4.attention.attention.query.weight\n",
      "encoder.layer.4.attention.attention.query.bias\n",
      "encoder.layer.4.attention.attention.key.weight\n",
      "encoder.layer.4.attention.attention.key.bias\n",
      "encoder.layer.4.attention.attention.value.weight\n",
      "encoder.layer.4.attention.attention.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.layernorm_before.weight\n",
      "encoder.layer.4.layernorm_before.bias\n",
      "encoder.layer.4.layernorm_after.weight\n",
      "encoder.layer.4.layernorm_after.bias\n",
      "encoder.layer.5.attention.attention.query.weight\n",
      "encoder.layer.5.attention.attention.query.bias\n",
      "encoder.layer.5.attention.attention.key.weight\n",
      "encoder.layer.5.attention.attention.key.bias\n",
      "encoder.layer.5.attention.attention.value.weight\n",
      "encoder.layer.5.attention.attention.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.layernorm_before.weight\n",
      "encoder.layer.5.layernorm_before.bias\n",
      "encoder.layer.5.layernorm_after.weight\n",
      "encoder.layer.5.layernorm_after.bias\n",
      "encoder.layer.6.attention.attention.query.weight\n",
      "encoder.layer.6.attention.attention.query.bias\n",
      "encoder.layer.6.attention.attention.key.weight\n",
      "encoder.layer.6.attention.attention.key.bias\n",
      "encoder.layer.6.attention.attention.value.weight\n",
      "encoder.layer.6.attention.attention.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.layernorm_before.weight\n",
      "encoder.layer.6.layernorm_before.bias\n",
      "encoder.layer.6.layernorm_after.weight\n",
      "encoder.layer.6.layernorm_after.bias\n",
      "encoder.layer.7.attention.attention.query.weight\n",
      "encoder.layer.7.attention.attention.query.bias\n",
      "encoder.layer.7.attention.attention.key.weight\n",
      "encoder.layer.7.attention.attention.key.bias\n",
      "encoder.layer.7.attention.attention.value.weight\n",
      "encoder.layer.7.attention.attention.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.layernorm_before.weight\n",
      "encoder.layer.7.layernorm_before.bias\n",
      "encoder.layer.7.layernorm_after.weight\n",
      "encoder.layer.7.layernorm_after.bias\n",
      "encoder.layer.8.attention.attention.query.weight\n",
      "encoder.layer.8.attention.attention.query.bias\n",
      "encoder.layer.8.attention.attention.key.weight\n",
      "encoder.layer.8.attention.attention.key.bias\n",
      "encoder.layer.8.attention.attention.value.weight\n",
      "encoder.layer.8.attention.attention.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.layernorm_before.weight\n",
      "encoder.layer.8.layernorm_before.bias\n",
      "encoder.layer.8.layernorm_after.weight\n",
      "encoder.layer.8.layernorm_after.bias\n",
      "encoder.layer.9.attention.attention.query.weight\n",
      "encoder.layer.9.attention.attention.query.bias\n",
      "encoder.layer.9.attention.attention.key.weight\n",
      "encoder.layer.9.attention.attention.key.bias\n",
      "encoder.layer.9.attention.attention.value.weight\n",
      "encoder.layer.9.attention.attention.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.layernorm_before.weight\n",
      "encoder.layer.9.layernorm_before.bias\n",
      "encoder.layer.9.layernorm_after.weight\n",
      "encoder.layer.9.layernorm_after.bias\n",
      "encoder.layer.10.attention.attention.query.weight\n",
      "encoder.layer.10.attention.attention.query.bias\n",
      "encoder.layer.10.attention.attention.key.weight\n",
      "encoder.layer.10.attention.attention.key.bias\n",
      "encoder.layer.10.attention.attention.value.weight\n",
      "encoder.layer.10.attention.attention.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.layernorm_before.weight\n",
      "encoder.layer.10.layernorm_before.bias\n",
      "encoder.layer.10.layernorm_after.weight\n",
      "encoder.layer.10.layernorm_after.bias\n",
      "encoder.layer.11.attention.attention.query.weight\n",
      "encoder.layer.11.attention.attention.query.bias\n",
      "encoder.layer.11.attention.attention.key.weight\n",
      "encoder.layer.11.attention.attention.key.bias\n",
      "encoder.layer.11.attention.attention.value.weight\n",
      "encoder.layer.11.attention.attention.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.layernorm_before.weight\n",
      "encoder.layer.11.layernorm_before.bias\n",
      "encoder.layer.11.layernorm_after.weight\n",
      "encoder.layer.11.layernorm_after.bias\n",
      "layernorm.weight\n",
      "layernorm.bias\n"
     ]
    }
   ],
   "source": [
    "for i,p in model.vit.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "371aa826",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = model.vit.embeddings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20ca1f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vit.embeddings.cls_token.requires_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb2fea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "1 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "2 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "3 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "4 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "5 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "6 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "7 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "8 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "9 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "10 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "11 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.vit.encoder.layer):\n",
    "    print(i, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddfe914d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 50, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a418e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a81cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c543de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.vit.encoder(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98795057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vit.layernorm(a.last_hidden_state)[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "821ebe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier(model.vit.layernorm(a.last_hidden_state)[:,0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66e8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
