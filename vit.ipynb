{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25af8812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 13:37:12.044406: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-06 13:37:12.692283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory\n",
      "2023-05-06 13:37:12.692933: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory\n",
      "2023-05-06 13:37:12.692941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Some weights of the model checkpoint at google/vit-base-patch32-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch32-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTModel, ViTConfig, ViTForImageClassification\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "cache_dir = \"/home/egeberk/ceng502/\"\n",
    "num_classes = 10\n",
    "model_name = 'google/vit-base-patch32-224-in21k'\n",
    "\n",
    "config = ViTConfig.from_pretrained(model_name, num_labels=num_classes, cache_dir=cache_dir)\n",
    "model = ViTForImageClassification.from_pretrained(model_name, config=config, cache_dir=cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b14a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c180874",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4ca498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the head\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "346e12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd09e88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-4\n",
    "batch_size = 124\n",
    "num_epochs = 10 \n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set the optimizer and loss function\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c5f578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(modelVit):\n",
    "    # Train the model\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = modelVit(data)\n",
    "        try:\n",
    "            loss = criterion(outputs.logits, targets)\n",
    "        except:\n",
    "            loss = criterion(outputs, targets)\n",
    "        loss_val = loss.detach().cpu().item()\n",
    "        train_loss += loss_val\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "    return train_loss/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087ee8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15753a5168104e90b6b8a3830800b17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0738051921719372\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b54d9103ef495ca9b592c557f1b15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.43248644522806207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d56ecf3addc47e2803f379e7a5d8f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.33499665012454044\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078a896dd5b34eee9e124a9edf9ce8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.2948971911158302\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81002d3323d445d297f1b27b14498a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.2710435390287992\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8f8e2bb2de4676a4a255153fdc69b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.2558640625586014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e662fa87e6475fac819242894ee45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.24442257984808766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7515922843534ee39336eee600f5c8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.23512031082617174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922df80fc70d435082562e74c0e579d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.22762656370454495\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb14d8e99cf458fa777d23389b9faa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.22125080657551194\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d783f4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f67536e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(modelVit):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(tqdm(valid_loader)):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = modelVit(data)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            try:\n",
    "                _, predicted = torch.max(outputs.logits, 1)\n",
    "            except:\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    return 100 * correct // total\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "104b83c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c1a9d8f5ec46c98fe197a187463cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 92 %\n"
     ]
    }
   ],
   "source": [
    "valid(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a96e5a",
   "metadata": {},
   "source": [
    "# Class + head fine tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3489e6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.embeddings.cls_token\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb43e58cc6f74b698e3dbd86a72a00b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0607944261467104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3efd5e008d4eda8011d0bf03310a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.3625623905053823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4685226cb246a9a27094ab3a4e761e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.2548982919664076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0344fb98d8ef403f8a12246d5db6f40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.2127182101185369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c8018c095346269ba7594f576cc006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.1896207969598841\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59624c03301d4c3ab7e8cf7b2807d1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.17512742105391946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25ca38706e24aac9025a0e7bdbbeb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.1644193258800424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346ae3741bae4b1a95a6810bc486012b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.1563596963181649\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8986f79449b049eea3c1be98efb40217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.14986024178232593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c7ee61f04147caba4e638497be9f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.14483140294652175\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "# Freeze all layers except the head\n",
    "for name, param in model.named_parameters():\n",
    "    if (\"classifier\" not in name) and (\"cls_token\" not in name) :\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        print(name)\n",
    "        \n",
    "        \n",
    "# Load and preprocess the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "learning_rate = 3e-4\n",
    "batch_size = 124\n",
    "num_epochs = 10 \n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set the optimizer and loss function\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407bd0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad28861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7ab03c1a334b83bf1e9229ad189584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 94 %\n"
     ]
    }
   ],
   "source": [
    "valid(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c02776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1826e977",
   "metadata": {},
   "source": [
    "# Memory Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd42d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Set, Tuple, Union\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8d71b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomViTLayerForMemory(nn.Module):\n",
    "    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig, attention, intermediate, output, layernorm_b, layernorm_a, memory_token) -> None:\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = attention #ViTAttention(config)\n",
    "        self.intermediate = intermediate #ViTIntermediate(config)\n",
    "        self.output = output #ViTOutput(config)\n",
    "        self.layernorm_before = layernorm_b #nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after = layernorm_a #nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "        self.memory_token = memory_token # torch.nn.Parameter(torch.randn(memory_token_length, embed_dim)) * 0.02\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        #memory_token: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        \n",
    "        original_states = hidden_states\n",
    "        \n",
    "        hidden_states = torch.cat((hidden_states, self.memory_token.expand(hidden_states.shape[0], -1, -1)), dim=1)\n",
    "        \n",
    "        self_attention_outputs = self.attention(\n",
    "            self.layernorm_before(hidden_states),  # in ViT, layernorm is applied before self-attention\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        \n",
    "        attention_output = self_attention_outputs[0][:,:-self.memory_token.shape[0]] #only input attends\n",
    "        \"\"\"\n",
    "        else:\n",
    "            attention_output = self_attention_outputs[0]\n",
    "        \"\"\"\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        # first residual connection\n",
    "        hidden_states = attention_output + original_states\n",
    "\n",
    "        # in ViT, layernorm is also applied after self-attention\n",
    "        layer_output = self.layernorm_after(hidden_states)\n",
    "        layer_output = self.intermediate(layer_output)\n",
    "\n",
    "        # second residual connection is done here\n",
    "        layer_output = self.output(layer_output, hidden_states)\n",
    "\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81fc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ef30117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperModel(nn.Module):\n",
    "    def __init__(self, config, vit, embed_dim = 768, memory_token_length = 10):\n",
    "        super(PaperModel, self).__init__()\n",
    "        self.memory_token_length = memory_token_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.model = vit\n",
    "        self.memory_token = nn.ParameterList([torch.nn.Parameter(torch.randn(self.memory_token_length, self.embed_dim) * 0.02)\n",
    "                                           for _ in range(len(self.model.vit.encoder.layer))])\n",
    "        for index, layer_ in enumerate(self.model.vit.encoder.layer):\n",
    "            self.model.vit.encoder.layer[index] = CustomViTLayerForMemory(config, \n",
    "                                          layer_.attention,\n",
    "                                          layer_.intermediate,\n",
    "                                          layer_.output,\n",
    "                                          layer_.layernorm_before,\n",
    "                                          layer_.layernorm_after,\n",
    "                                         self.memory_token[index])\n",
    "            \n",
    "            \n",
    "        \n",
    "        #self.memory_token = torch.nn.Parameter(\n",
    "        #    torch.randn(self.memory_token_length, self.embed_dim)\n",
    "        #) #memory token\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden_states = self.model.vit.embeddings(x)\n",
    "        \n",
    "        for layer in self.model.vit.encoder.layer:\n",
    "            hidden_states = layer(hidden_states)[0]#layer(hidden_states, self.memory_token)[0]            \n",
    "        \n",
    "        normalized = self.model.vit.layernorm(hidden_states)\n",
    "        logits = self.model.classifier(normalized[:, 0, :])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32d96c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = PaperModel(config, model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8948ae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.vit.embeddings.cls_token\n",
      "model.vit.encoder.layer.0.memory_token\n",
      "model.vit.encoder.layer.1.memory_token\n",
      "model.vit.encoder.layer.2.memory_token\n",
      "model.vit.encoder.layer.3.memory_token\n",
      "model.vit.encoder.layer.4.memory_token\n",
      "model.vit.encoder.layer.5.memory_token\n",
      "model.vit.encoder.layer.6.memory_token\n",
      "model.vit.encoder.layer.7.memory_token\n",
      "model.vit.encoder.layer.8.memory_token\n",
      "model.vit.encoder.layer.9.memory_token\n",
      "model.vit.encoder.layer.10.memory_token\n",
      "model.vit.encoder.layer.11.memory_token\n",
      "model.classifier.weight\n",
      "model.classifier.bias\n"
     ]
    }
   ],
   "source": [
    "#memory + cls + head\n",
    "for name, param in pm.named_parameters():\n",
    "    if (\"classifier\" not in name) and (\"cls_token\" not in name) and (\"memory_token\" not in name):\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99996012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizer and loss function\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, pm.parameters()), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e03d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59fd5daf9cf64bb0bb1b8a9cce087f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 0.4820975792216192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e27af4cdd69413a89c5da85d1e5861f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 96 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3ee48707e84865bd19d016c4f52fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Loss: 0.09054598888834145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa9e8bfef3c4340be5d653299020cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8fe78c64b046a48588cd173cc06d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300], Loss: 0.06677886168472469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb35423081042218be045718de67190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6e90674e834861b3ca1829a2758edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300], Loss: 0.05176729497057155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a588814617442239ff0a67fabfa32ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91aa5053fec1405181b03d48f45bd099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300], Loss: 0.045392660817735374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5904cb3df85942a7ba20accc022c015b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024548dadd374fa2a21228e289d9b37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300], Loss: 0.03684841303009595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ce33f2350e47378e0bcb7b66ec1f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640d5e02a1214fe1a1b97247aa7d58ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/300], Loss: 0.030220185592655314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638a133f89da48c0b3394afed677112d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c5b9e8b81d4caa82bb025d267c00f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300], Loss: 0.02457816515574324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d0610f33724579a723c9cd9f56ce24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e709d825538d4b568ee921116d411c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/300], Loss: 0.021791133649836265\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fb6109445f4b1289b22a3a9d12c938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba3221b86884f08bff9420452ccda04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Loss: 0.017969800122306995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c613b0773446a89a763a2a54717504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f58663ba08446eabe01e75c503fb12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/300], Loss: 0.014483976960216768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffa090ad6494378a65a6c27b0fbcdde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b08b7bed9a4d7c87bf0a2e2040454c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/300], Loss: 0.01172283375756769\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a08b4818d14db8b0ed17c4fb515399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0d1a4929cc4dbb84ffd2d939f3c106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/300], Loss: 0.010527419917761478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80be9d2908924ac7a83ae7cc862dffe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d45b4a21ea481ebd5f42c190194598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "train_losses = []\n",
    "acc_vals = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(pm)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss}\")\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    acc_val = valid(pm)\n",
    "    \n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc_val} %')\n",
    "    acc_vals.append(acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "412c38e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0987822932772118,\n",
       " 1.7861921536450338,\n",
       " 1.5670567500119161,\n",
       " 1.4042141891942166,\n",
       " 1.2790964316613604,\n",
       " 1.180181681077079,\n",
       " 1.1007926406541673,\n",
       " 1.0349034649310727,\n",
       " 0.9799850758644614,\n",
       " 0.9323421806687175,\n",
       " 0.89216248809111,\n",
       " 0.8562249011627519,\n",
       " 0.8251895919294641,\n",
       " 0.797393385431554,\n",
       " 0.7721824296335182,\n",
       " 0.7494477324261524,\n",
       " 0.7291482809451547,\n",
       " 0.7103912710848421,\n",
       " 0.6936880566991201,\n",
       " 0.6776619774870353,\n",
       " 0.6631273078446341,\n",
       " 0.6502166125591439,\n",
       " 0.638310113608247,\n",
       " 0.6269282804857387,\n",
       " 0.615705787840456,\n",
       " 0.6054490137808394,\n",
       " 0.5963591269337305,\n",
       " 0.5878394600926059,\n",
       " 0.5788741146426389,\n",
       " 0.5711343225718725,\n",
       " 0.5637091265456511,\n",
       " 0.5570303041598584,\n",
       " 0.5499377931551178,\n",
       " 0.5435361485965181,\n",
       " 0.5371362156059483,\n",
       " 0.5313240162069255,\n",
       " 0.5258289616886932,\n",
       " 0.5205286705110332,\n",
       " 0.5159834059010638,\n",
       " 0.5109650904294287,\n",
       " 0.5059272140116975,\n",
       " 0.5013424431038375,\n",
       " 0.49807981335290585,\n",
       " 0.49250793659893594,\n",
       " 0.48881919158272225,\n",
       " 0.4847982216737058,\n",
       " 0.48116562087642084,\n",
       " 0.4776037395147994,\n",
       " 0.47410848065473066,\n",
       " 0.4709891815586845,\n",
       " 0.4679641826908187,\n",
       " 0.464590091590244,\n",
       " 0.4610188871917158,\n",
       " 0.4590542348894742,\n",
       " 0.4549332019863742,\n",
       " 0.45319294936881205,\n",
       " 0.45008126712671603,\n",
       " 0.44734850038986396,\n",
       " 0.4446190951189192,\n",
       " 0.44228477746543315,\n",
       " 0.43962882370641915,\n",
       " 0.43719349292540316,\n",
       " 0.43505841759171815,\n",
       " 0.43236545650380676,\n",
       " 0.4301881512792984,\n",
       " 0.42807722423631367,\n",
       " 0.4262723056691708,\n",
       " 0.4243427162418271,\n",
       " 0.42196696790138094,\n",
       " 0.4197181880769163,\n",
       " 0.4180729319110955,\n",
       " 0.41590207767221005,\n",
       " 0.41403980652737143,\n",
       " 0.4121720547059385,\n",
       " 0.4107497535129585,\n",
       " 0.40910464399816965,\n",
       " 0.40739781874241215,\n",
       " 0.40525211224166474,\n",
       " 0.4039336589745956,\n",
       " 0.4020511605730741,\n",
       " 0.4009509314921233,\n",
       " 0.3991539681164345,\n",
       " 0.3976240976538398,\n",
       " 0.3960786781246119,\n",
       " 0.3948159439138847,\n",
       " 0.39311573661790034,\n",
       " 0.39208597262011896,\n",
       " 0.39046307461391583,\n",
       " 0.3889529560462083,\n",
       " 0.38804707923295473,\n",
       " 0.3865228503352345,\n",
       " 0.3847868469091925,\n",
       " 0.38423726324102664,\n",
       " 0.38254216446144745,\n",
       " 0.38128154680575477,\n",
       " 0.37997921139444457,\n",
       " 0.3790193079500505,\n",
       " 0.37818571065764617,\n",
       " 0.3769684449428379,\n",
       " 0.37531943977026655]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "369489cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 76,\n",
       " 77,\n",
       " 77,\n",
       " 78,\n",
       " 78,\n",
       " 79,\n",
       " 79,\n",
       " 79,\n",
       " 80,\n",
       " 80,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 82,\n",
       " 82,\n",
       " 82,\n",
       " 82,\n",
       " 82,\n",
       " 83,\n",
       " 83,\n",
       " 83,\n",
       " 83,\n",
       " 83,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 85,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 87,\n",
       " 86,\n",
       " 86,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e1a077f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9677eedfcd148bf9af663e09d555413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 7.79 GiB total capacity; 5.03 GiB already allocated; 23.12 MiB free; 5.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 2\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(modelVit)\u001b[0m\n\u001b[1;32m      8\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodelVit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mlogits, targets)\n",
      "File \u001b[0;32m~/ege/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m, in \u001b[0;36mPaperModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvit\u001b[38;5;241m.\u001b[39membeddings(x)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvit\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m---> 29\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;66;03m#layer(hidden_states, self.memory_token)[0]            \u001b[39;00m\n\u001b[1;32m     31\u001b[0m normalized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvit\u001b[38;5;241m.\u001b[39mlayernorm(hidden_states)\n\u001b[1;32m     32\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mclassifier(normalized[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[0;32m~/ege/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[11], line 46\u001b[0m, in \u001b[0;36mCustomViTLayerForMemory.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# in ViT, layernorm is also applied after self-attention\u001b[39;00m\n\u001b[1;32m     45\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_after(hidden_states)\n\u001b[0;32m---> 46\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# second residual connection is done here\u001b[39;00m\n\u001b[1;32m     49\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(layer_output, hidden_states)\n",
      "File \u001b[0;32m~/ege/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ege/lib/python3.10/site-packages/transformers/models/vit/modeling_vit.py:320\u001b[0m, in \u001b[0;36mViTIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    319\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 320\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/ege/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ege/lib/python3.10/site-packages/transformers/activations.py:57\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 7.79 GiB total capacity; 5.03 GiB already allocated; 23.12 MiB free; 5.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(pm)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss}\")\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    acc_val = valid(pm)\n",
    "    \n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc_val} %')\n",
    "    acc_vals.append(acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a955281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef798d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8dbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e62136e",
   "metadata": {},
   "source": [
    "No memory token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a0d5b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.vit.embeddings.cls_token\n",
      "model.classifier.weight\n",
      "model.classifier.bias\n"
     ]
    }
   ],
   "source": [
    "class CustomViTLayerForMemory(nn.Module):\n",
    "    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig, attention, intermediate, output, layernorm_b, layernorm_a)-> None:#, memory_token) -> None:\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = attention #ViTAttention(config)\n",
    "        self.intermediate = intermediate #ViTIntermediate(config)\n",
    "        self.output = output #ViTOutput(config)\n",
    "        self.layernorm_before = layernorm_b #nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after = layernorm_a #nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "        #self.memory_token = memory_token # torch.nn.Parameter(torch.randn(memory_token_length, embed_dim)) * 0.02\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        #memory_token: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        \n",
    "        original_states = hidden_states\n",
    "        \n",
    "        #hidden_states = torch.cat((hidden_states, self.memory_token.expand(hidden_states.shape[0], -1, -1)), dim=1)\n",
    "        \n",
    "        self_attention_outputs = self.attention(\n",
    "            self.layernorm_before(hidden_states),  # in ViT, layernorm is applied before self-attention\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        \n",
    "        attention_output = self_attention_outputs[0]#[:,:-self.memory_token.shape[0]] #only input attends\n",
    "        \"\"\"\n",
    "        else:\n",
    "            attention_output = self_attention_outputs[0]\n",
    "        \"\"\"\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        # first residual connection\n",
    "        hidden_states = attention_output + original_states\n",
    "\n",
    "        # in ViT, layernorm is also applied after self-attention\n",
    "        layer_output = self.layernorm_after(hidden_states)\n",
    "        layer_output = self.intermediate(layer_output)\n",
    "\n",
    "        # second residual connection is done here\n",
    "        layer_output = self.output(layer_output, hidden_states)\n",
    "\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PaperModel(nn.Module):\n",
    "    def __init__(self, config, vit, embed_dim = 768):#, memory_token_length = 10):\n",
    "        super(PaperModel, self).__init__()\n",
    "        #self.memory_token_length = memory_token_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.model = vit\n",
    "        #self.memory_token = nn.ParameterList([torch.nn.Parameter(torch.randn(self.memory_token_length, self.embed_dim) * 0.02)\n",
    "        #                                   for _ in range(len(self.model.vit.encoder.layer))])\n",
    "        for index, layer_ in enumerate(self.model.vit.encoder.layer):\n",
    "            self.model.vit.encoder.layer[index] = CustomViTLayerForMemory(config, \n",
    "                                          layer_.attention,\n",
    "                                          layer_.intermediate,\n",
    "                                          layer_.output,\n",
    "                                          layer_.layernorm_before,\n",
    "                                          layer_.layernorm_after)#,\n",
    "                                         #self.memory_token[index])\n",
    "            \n",
    "            \n",
    "        \n",
    "        #self.memory_token = torch.nn.Parameter(\n",
    "        #    torch.randn(self.memory_token_length, self.embed_dim)\n",
    "        #) #memory token\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden_states = self.model.vit.embeddings(x)\n",
    "        \n",
    "        for layer in self.model.vit.encoder.layer:\n",
    "            hidden_states = layer(hidden_states)[0]#layer(hidden_states, self.memory_token)[0]            \n",
    "        \n",
    "        normalized = self.model.vit.layernorm(hidden_states)\n",
    "        logits = self.model.classifier(normalized[:, 0, :])\n",
    "        return logits\n",
    "\n",
    "pm = PaperModel(config, model).to(device)\n",
    "\n",
    "#memory + cls + head\n",
    "for name, param in pm.named_parameters():\n",
    "    if (\"classifier\" not in name) and (\"cls_token\" not in name) and (\"memory_token\" not in name):\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "223b45dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67d2122f32343a48f33571fe341c72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0749211574692537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab15561f776a4a2bb06b568c811a0a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.43293110443518895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dc35803c614d63a1fe146f0bc8faba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.3343392747758639\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9400caadc56840f6a9693c4b6f90da99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.29389287282408466\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764b27c369e94b438809e8dbb2e99238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.27153268386379326\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e514741700448b91a268bcbc898d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.25559217267033485\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b6c121980c42afb3503bb1b70807b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.23523561197268492\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206179c954234356a93192cb2491e837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.22781454978307875\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9908917ab98342a69a003fa704e03980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.221015185385131\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0613acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa7f9b1a9184dee95f5138661100faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 92 %\n"
     ]
    }
   ],
   "source": [
    "valid(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08872a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452273f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f04313c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c8cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb34dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50555d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89f62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0c41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c38df16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c02ce42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48add188a0c4773867d19af0b4e83e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.27229671062219263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bc2ff5719f47198b022556710ed2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.2285295785543057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789f26a17290456bbe33c70420deacc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.21214430391936018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5de5b8f2d744670922ca820677a9673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.20146242372396558\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34979c428c704187aa71e0f310664616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.19165904060153677\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0428d07e35438e83f301a07feec721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.18456119456614303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9a91d1022f42ddb709c9fa1e6f6752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.1778454251107898\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983ea705526f4f998f609d6f031cf048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.1725820623670179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d6dbec81ad45a7874defa5d05c6473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.16802955144960988\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361770e39c814a3b85ecbcd6cd805934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.16291090112529916\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "train(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "810d7513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590f55755d894f9795e49dfeb8c151d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 93 %\n"
     ]
    }
   ],
   "source": [
    "valid(pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa3076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10a5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "48ddf704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a731ad5ccf824cb695f0976056750e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_idx, (data, targets) in enumerate(tqdm(valid_loader)):\n",
    "    data = data.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3cc8c356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0278, -0.8870,  1.5921,  ..., -1.4284, -0.4906, -0.6489],\n",
       "        [ 0.3318, -0.6451,  1.4786,  ..., -1.2412,  0.2162, -0.3718],\n",
       "        [ 0.2759, -0.6854,  1.3918,  ..., -1.2371,  0.3058, -0.3987],\n",
       "        ...,\n",
       "        [-0.0230, -0.9826,  1.5481,  ..., -1.1421, -0.3711, -0.2848],\n",
       "        [-0.1069, -0.4906,  1.4808,  ..., -1.5035, -0.3257, -0.0312],\n",
       "        [ 0.1012, -1.0433,  2.1145,  ..., -1.3779, -0.4260, -0.6846]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd316029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customAttentionWithMemory(nn.Module):\n",
    "    def __init__(self, config: ViTConfig, query, key, value, dropout) -> None:\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n",
    "                f\"heads {config.num_attention_heads}.\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = query\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self, q, k, v):\n",
    "        mixed_query_layer = self.query(q)\n",
    "\n",
    "        key_layer = self.transpose_for_scores(self.key(k))\n",
    "        value_layer = self.transpose_for_scores(self.value(v))\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c759bb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 17 (1751082145.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[72], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    att_out = self.layer(x, self.number_of_head)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 17\n"
     ]
    }
   ],
   "source": [
    "class PaperModel(nn.Module):\n",
    "    def __init__(self, vit, embed_dim = 768, memory_token_length = 10):\n",
    "        super(PaperModel, self).__init__()\n",
    "        self.memory_token_length = memory_token_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.model = vit\n",
    "        self.memory_token = torch.nn.Parameter(\n",
    "            torch.randn(self.memory_token_length, self.embed_dim)\n",
    "        ) #memory token\n",
    "        for layer in self.model.vit.encoder:\n",
    "            att_model = layer.attention\n",
    "            layer.attention = customAttentionWithMemory(config, att_model.attention.query,\\\n",
    "                                                        att_model.attention.key, att_model.attention.value,att_model.attention.dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        input = self.model.vit.embeddings(x)\n",
    "        for layer in self.model.vit.encoder.layer:\n",
    "            \n",
    "        \n",
    "        att_out = self.layer(x, self.number_of_head)\n",
    "        att_residual_out = att_out + x\n",
    "        norm1_out = self.dropout1(self.norm1(att_residual_out))\n",
    "        ff_out = self.MLP_sequence(norm1_out)\n",
    "        ff_res_out = ff_out + norm1_out\n",
    "        norm2_out = self.dropout2(self.norm2(ff_res_out))\n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be85f367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed052e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.1750,  1.2434,  0.0246,  ...,  1.0854,  0.5624,  1.8326],\n",
       "          [ 0.5839,  0.4748, -0.9245,  ...,  0.4982,  0.0449,  1.3741],\n",
       "          [-0.5252,  1.6358, -0.8340,  ...,  0.5063,  0.6809,  1.2358],\n",
       "          ...,\n",
       "          [ 1.5790,  1.5951, -0.5681,  ..., -0.0584,  0.5637,  0.7831],\n",
       "          [ 0.4778,  0.8916, -0.3458,  ...,  0.7740,  0.7255,  1.5669],\n",
       "          [-0.1720,  2.2242,  0.1825,  ...,  0.7067,  0.1703,  1.1183]],\n",
       " \n",
       "         [[ 0.0317,  1.9712, -1.1072,  ...,  1.0647,  0.2739,  1.5160],\n",
       "          [-0.5021,  2.0634, -1.3776,  ...,  0.7768,  0.0027,  1.1941],\n",
       "          [ 0.5766,  0.7833, -0.5032,  ...,  0.6730, -0.1111,  0.7818],\n",
       "          ...,\n",
       "          [ 0.6456,  1.2389, -1.0500,  ...,  0.8230, -0.2533,  1.4723],\n",
       "          [-0.2427,  0.3021, -1.5380,  ...,  0.9418, -0.4302,  0.5285],\n",
       "          [ 0.6844,  1.5814, -0.3445,  ...,  0.5765, -0.6269,  1.1135]],\n",
       " \n",
       "         [[ 0.0652,  0.3140, -0.6031,  ...,  0.4982,  1.1300,  2.2200],\n",
       "          [ 0.7496,  1.8503, -0.3597,  ...,  0.4807,  0.8226,  0.8485],\n",
       "          [ 0.1290,  1.8189, -1.1795,  ...,  0.4288,  0.5787,  1.2097],\n",
       "          ...,\n",
       "          [ 0.2937,  1.5051, -1.1848,  ...,  0.6211,  0.4715,  1.1732],\n",
       "          [ 0.1129,  2.3061, -0.2484,  ...,  0.4988, -0.0814,  1.7406],\n",
       "          [ 0.0974,  1.3779, -0.8254,  ...,  0.2124, -0.5762,  0.5515]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.3216,  1.5224, -0.9534,  ...,  1.2367,  0.7969,  1.2935],\n",
       "          [ 0.3551,  0.8337, -1.0850,  ...,  0.1682,  0.4069, -0.1397],\n",
       "          [ 0.6770,  1.2924, -0.6037,  ...,  0.6356, -0.4211,  0.3495],\n",
       "          ...,\n",
       "          [-0.4930,  0.9827, -0.5448,  ...,  0.8914,  0.0974,  1.2046],\n",
       "          [ 0.3798,  1.8572, -0.8579,  ...,  0.3174,  0.6701,  1.5018],\n",
       "          [ 1.0607,  0.5246, -0.2792,  ...,  0.7054,  0.7901, -0.2080]],\n",
       " \n",
       "         [[ 0.0965,  1.5849,  0.0803,  ...,  0.6281, -0.7096,  1.4393],\n",
       "          [ 0.7310,  1.5263, -1.1805,  ...,  1.0038, -0.0543, -0.4456],\n",
       "          [ 0.2883,  1.0404, -1.1583,  ...,  0.1801,  1.2232,  1.7764],\n",
       "          ...,\n",
       "          [ 0.2465,  1.4299, -0.8948,  ...,  0.7821,  0.5200,  1.3264],\n",
       "          [-0.2761,  0.8486, -0.6597,  ...,  0.6752,  0.2748,  2.2627],\n",
       "          [-0.2453,  1.4308, -0.7784,  ...,  0.4823,  0.7658,  1.5281]],\n",
       " \n",
       "         [[ 0.4167,  0.9356, -1.3095,  ...,  0.3280,  0.1157,  0.6103],\n",
       "          [ 0.9634,  1.3174, -0.1172,  ...,  0.9718, -0.1917,  1.8015],\n",
       "          [ 0.0054,  1.0056, -0.6831,  ...,  0.5208,  0.6431,  1.2557],\n",
       "          ...,\n",
       "          [ 0.4376,  2.2553, -0.7913,  ...,  0.8399,  0.5072,  0.5553],\n",
       "          [ 0.6856,  0.7474,  0.1166,  ...,  0.5667,  0.3252,  1.4412],\n",
       "          [ 0.4203,  0.4814, -0.2715,  ...,  0.7466, -0.2142,  1.4803]]],\n",
       "        device='cuda:0'),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vit.encoder.layer[0](k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbae2231",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand([28, 50, 768])\n",
    "expanded_memory = torch.randn((10, 768))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "121cff1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 10, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_memory.expand(28, -1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbde6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.cat((q, expanded_memory.expand(28, -1, -1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fb1494d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 60, 768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f15efeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 50, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:,:-expanded_memory.shape[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af9dab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(s.shape).to(device)\n",
    "k = torch.rand([28, 60, 768]).to(device)\n",
    "v = torch.rand([28, 60, 768]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fecb7cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a = model.vit.encoder.layer[0].attention.attention.query(q)\n",
    "k_a = model.vit.encoder.layer[0].attention.attention.key(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a64c27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = q.matmul(k.transpose(-1, -2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79282b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTLayer(\n",
       "  (attention): ViTAttention(\n",
       "    (attention): ViTSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (output): ViTSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): ViTIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): ViTOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_ = model.vit.encoder.layer[0]\n",
    "layer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e71120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_layer_ = CustomViTLayerForMemory(config, layer_.attention, layer_.intermediate, layer_.output, layer_.layernorm_before, layer_.layernorm_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8add7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_layer_ = custom_layer_.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70beec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q.to(device)\n",
    "expanded_memory = expanded_memory.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23c6bea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 50, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_layer_(q, expanded_memory)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bc0261c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a15f5d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 50, 60])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f9a270bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_model = model.vit.encoder.layer[0].attention\n",
    "\n",
    "ca = customAttentionWithMemory(config, att_model.attention.query, att_model.attention.key, att_model.attention.value, att_model.attention.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e5d8d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 60, 768])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "725bf7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 50, 768])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca(q,k,v)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b35ad80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vit.config.attention_probs_dropout_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9bb433a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTLayer(\n",
       "  (attention): ViTAttention(\n",
       "    (attention): ViTSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (output): ViTSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): ViTIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): ViTOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vit.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430fffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a41336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a710ae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit.embeddings.cls_token\n",
      "vit.embeddings.position_embeddings\n",
      "vit.embeddings.patch_embeddings.projection.weight\n",
      "vit.embeddings.patch_embeddings.projection.bias\n",
      "vit.encoder.layer.0.attention.attention.query.weight\n",
      "vit.encoder.layer.0.attention.attention.query.bias\n",
      "vit.encoder.layer.0.attention.attention.key.weight\n",
      "vit.encoder.layer.0.attention.attention.key.bias\n",
      "vit.encoder.layer.0.attention.attention.value.weight\n",
      "vit.encoder.layer.0.attention.attention.value.bias\n",
      "vit.encoder.layer.0.attention.output.dense.weight\n",
      "vit.encoder.layer.0.attention.output.dense.bias\n",
      "vit.encoder.layer.0.intermediate.dense.weight\n",
      "vit.encoder.layer.0.intermediate.dense.bias\n",
      "vit.encoder.layer.0.output.dense.weight\n",
      "vit.encoder.layer.0.output.dense.bias\n",
      "vit.encoder.layer.0.layernorm_before.weight\n",
      "vit.encoder.layer.0.layernorm_before.bias\n",
      "vit.encoder.layer.0.layernorm_after.weight\n",
      "vit.encoder.layer.0.layernorm_after.bias\n",
      "vit.encoder.layer.1.attention.attention.query.weight\n",
      "vit.encoder.layer.1.attention.attention.query.bias\n",
      "vit.encoder.layer.1.attention.attention.key.weight\n",
      "vit.encoder.layer.1.attention.attention.key.bias\n",
      "vit.encoder.layer.1.attention.attention.value.weight\n",
      "vit.encoder.layer.1.attention.attention.value.bias\n",
      "vit.encoder.layer.1.attention.output.dense.weight\n",
      "vit.encoder.layer.1.attention.output.dense.bias\n",
      "vit.encoder.layer.1.intermediate.dense.weight\n",
      "vit.encoder.layer.1.intermediate.dense.bias\n",
      "vit.encoder.layer.1.output.dense.weight\n",
      "vit.encoder.layer.1.output.dense.bias\n",
      "vit.encoder.layer.1.layernorm_before.weight\n",
      "vit.encoder.layer.1.layernorm_before.bias\n",
      "vit.encoder.layer.1.layernorm_after.weight\n",
      "vit.encoder.layer.1.layernorm_after.bias\n",
      "vit.encoder.layer.2.attention.attention.query.weight\n",
      "vit.encoder.layer.2.attention.attention.query.bias\n",
      "vit.encoder.layer.2.attention.attention.key.weight\n",
      "vit.encoder.layer.2.attention.attention.key.bias\n",
      "vit.encoder.layer.2.attention.attention.value.weight\n",
      "vit.encoder.layer.2.attention.attention.value.bias\n",
      "vit.encoder.layer.2.attention.output.dense.weight\n",
      "vit.encoder.layer.2.attention.output.dense.bias\n",
      "vit.encoder.layer.2.intermediate.dense.weight\n",
      "vit.encoder.layer.2.intermediate.dense.bias\n",
      "vit.encoder.layer.2.output.dense.weight\n",
      "vit.encoder.layer.2.output.dense.bias\n",
      "vit.encoder.layer.2.layernorm_before.weight\n",
      "vit.encoder.layer.2.layernorm_before.bias\n",
      "vit.encoder.layer.2.layernorm_after.weight\n",
      "vit.encoder.layer.2.layernorm_after.bias\n",
      "vit.encoder.layer.3.attention.attention.query.weight\n",
      "vit.encoder.layer.3.attention.attention.query.bias\n",
      "vit.encoder.layer.3.attention.attention.key.weight\n",
      "vit.encoder.layer.3.attention.attention.key.bias\n",
      "vit.encoder.layer.3.attention.attention.value.weight\n",
      "vit.encoder.layer.3.attention.attention.value.bias\n",
      "vit.encoder.layer.3.attention.output.dense.weight\n",
      "vit.encoder.layer.3.attention.output.dense.bias\n",
      "vit.encoder.layer.3.intermediate.dense.weight\n",
      "vit.encoder.layer.3.intermediate.dense.bias\n",
      "vit.encoder.layer.3.output.dense.weight\n",
      "vit.encoder.layer.3.output.dense.bias\n",
      "vit.encoder.layer.3.layernorm_before.weight\n",
      "vit.encoder.layer.3.layernorm_before.bias\n",
      "vit.encoder.layer.3.layernorm_after.weight\n",
      "vit.encoder.layer.3.layernorm_after.bias\n",
      "vit.encoder.layer.4.attention.attention.query.weight\n",
      "vit.encoder.layer.4.attention.attention.query.bias\n",
      "vit.encoder.layer.4.attention.attention.key.weight\n",
      "vit.encoder.layer.4.attention.attention.key.bias\n",
      "vit.encoder.layer.4.attention.attention.value.weight\n",
      "vit.encoder.layer.4.attention.attention.value.bias\n",
      "vit.encoder.layer.4.attention.output.dense.weight\n",
      "vit.encoder.layer.4.attention.output.dense.bias\n",
      "vit.encoder.layer.4.intermediate.dense.weight\n",
      "vit.encoder.layer.4.intermediate.dense.bias\n",
      "vit.encoder.layer.4.output.dense.weight\n",
      "vit.encoder.layer.4.output.dense.bias\n",
      "vit.encoder.layer.4.layernorm_before.weight\n",
      "vit.encoder.layer.4.layernorm_before.bias\n",
      "vit.encoder.layer.4.layernorm_after.weight\n",
      "vit.encoder.layer.4.layernorm_after.bias\n",
      "vit.encoder.layer.5.attention.attention.query.weight\n",
      "vit.encoder.layer.5.attention.attention.query.bias\n",
      "vit.encoder.layer.5.attention.attention.key.weight\n",
      "vit.encoder.layer.5.attention.attention.key.bias\n",
      "vit.encoder.layer.5.attention.attention.value.weight\n",
      "vit.encoder.layer.5.attention.attention.value.bias\n",
      "vit.encoder.layer.5.attention.output.dense.weight\n",
      "vit.encoder.layer.5.attention.output.dense.bias\n",
      "vit.encoder.layer.5.intermediate.dense.weight\n",
      "vit.encoder.layer.5.intermediate.dense.bias\n",
      "vit.encoder.layer.5.output.dense.weight\n",
      "vit.encoder.layer.5.output.dense.bias\n",
      "vit.encoder.layer.5.layernorm_before.weight\n",
      "vit.encoder.layer.5.layernorm_before.bias\n",
      "vit.encoder.layer.5.layernorm_after.weight\n",
      "vit.encoder.layer.5.layernorm_after.bias\n",
      "vit.encoder.layer.6.attention.attention.query.weight\n",
      "vit.encoder.layer.6.attention.attention.query.bias\n",
      "vit.encoder.layer.6.attention.attention.key.weight\n",
      "vit.encoder.layer.6.attention.attention.key.bias\n",
      "vit.encoder.layer.6.attention.attention.value.weight\n",
      "vit.encoder.layer.6.attention.attention.value.bias\n",
      "vit.encoder.layer.6.attention.output.dense.weight\n",
      "vit.encoder.layer.6.attention.output.dense.bias\n",
      "vit.encoder.layer.6.intermediate.dense.weight\n",
      "vit.encoder.layer.6.intermediate.dense.bias\n",
      "vit.encoder.layer.6.output.dense.weight\n",
      "vit.encoder.layer.6.output.dense.bias\n",
      "vit.encoder.layer.6.layernorm_before.weight\n",
      "vit.encoder.layer.6.layernorm_before.bias\n",
      "vit.encoder.layer.6.layernorm_after.weight\n",
      "vit.encoder.layer.6.layernorm_after.bias\n",
      "vit.encoder.layer.7.attention.attention.query.weight\n",
      "vit.encoder.layer.7.attention.attention.query.bias\n",
      "vit.encoder.layer.7.attention.attention.key.weight\n",
      "vit.encoder.layer.7.attention.attention.key.bias\n",
      "vit.encoder.layer.7.attention.attention.value.weight\n",
      "vit.encoder.layer.7.attention.attention.value.bias\n",
      "vit.encoder.layer.7.attention.output.dense.weight\n",
      "vit.encoder.layer.7.attention.output.dense.bias\n",
      "vit.encoder.layer.7.intermediate.dense.weight\n",
      "vit.encoder.layer.7.intermediate.dense.bias\n",
      "vit.encoder.layer.7.output.dense.weight\n",
      "vit.encoder.layer.7.output.dense.bias\n",
      "vit.encoder.layer.7.layernorm_before.weight\n",
      "vit.encoder.layer.7.layernorm_before.bias\n",
      "vit.encoder.layer.7.layernorm_after.weight\n",
      "vit.encoder.layer.7.layernorm_after.bias\n",
      "vit.encoder.layer.8.attention.attention.query.weight\n",
      "vit.encoder.layer.8.attention.attention.query.bias\n",
      "vit.encoder.layer.8.attention.attention.key.weight\n",
      "vit.encoder.layer.8.attention.attention.key.bias\n",
      "vit.encoder.layer.8.attention.attention.value.weight\n",
      "vit.encoder.layer.8.attention.attention.value.bias\n",
      "vit.encoder.layer.8.attention.output.dense.weight\n",
      "vit.encoder.layer.8.attention.output.dense.bias\n",
      "vit.encoder.layer.8.intermediate.dense.weight\n",
      "vit.encoder.layer.8.intermediate.dense.bias\n",
      "vit.encoder.layer.8.output.dense.weight\n",
      "vit.encoder.layer.8.output.dense.bias\n",
      "vit.encoder.layer.8.layernorm_before.weight\n",
      "vit.encoder.layer.8.layernorm_before.bias\n",
      "vit.encoder.layer.8.layernorm_after.weight\n",
      "vit.encoder.layer.8.layernorm_after.bias\n",
      "vit.encoder.layer.9.attention.attention.query.weight\n",
      "vit.encoder.layer.9.attention.attention.query.bias\n",
      "vit.encoder.layer.9.attention.attention.key.weight\n",
      "vit.encoder.layer.9.attention.attention.key.bias\n",
      "vit.encoder.layer.9.attention.attention.value.weight\n",
      "vit.encoder.layer.9.attention.attention.value.bias\n",
      "vit.encoder.layer.9.attention.output.dense.weight\n",
      "vit.encoder.layer.9.attention.output.dense.bias\n",
      "vit.encoder.layer.9.intermediate.dense.weight\n",
      "vit.encoder.layer.9.intermediate.dense.bias\n",
      "vit.encoder.layer.9.output.dense.weight\n",
      "vit.encoder.layer.9.output.dense.bias\n",
      "vit.encoder.layer.9.layernorm_before.weight\n",
      "vit.encoder.layer.9.layernorm_before.bias\n",
      "vit.encoder.layer.9.layernorm_after.weight\n",
      "vit.encoder.layer.9.layernorm_after.bias\n",
      "vit.encoder.layer.10.attention.attention.query.weight\n",
      "vit.encoder.layer.10.attention.attention.query.bias\n",
      "vit.encoder.layer.10.attention.attention.key.weight\n",
      "vit.encoder.layer.10.attention.attention.key.bias\n",
      "vit.encoder.layer.10.attention.attention.value.weight\n",
      "vit.encoder.layer.10.attention.attention.value.bias\n",
      "vit.encoder.layer.10.attention.output.dense.weight\n",
      "vit.encoder.layer.10.attention.output.dense.bias\n",
      "vit.encoder.layer.10.intermediate.dense.weight\n",
      "vit.encoder.layer.10.intermediate.dense.bias\n",
      "vit.encoder.layer.10.output.dense.weight\n",
      "vit.encoder.layer.10.output.dense.bias\n",
      "vit.encoder.layer.10.layernorm_before.weight\n",
      "vit.encoder.layer.10.layernorm_before.bias\n",
      "vit.encoder.layer.10.layernorm_after.weight\n",
      "vit.encoder.layer.10.layernorm_after.bias\n",
      "vit.encoder.layer.11.attention.attention.query.weight\n",
      "vit.encoder.layer.11.attention.attention.query.bias\n",
      "vit.encoder.layer.11.attention.attention.key.weight\n",
      "vit.encoder.layer.11.attention.attention.key.bias\n",
      "vit.encoder.layer.11.attention.attention.value.weight\n",
      "vit.encoder.layer.11.attention.attention.value.bias\n",
      "vit.encoder.layer.11.attention.output.dense.weight\n",
      "vit.encoder.layer.11.attention.output.dense.bias\n",
      "vit.encoder.layer.11.intermediate.dense.weight\n",
      "vit.encoder.layer.11.intermediate.dense.bias\n",
      "vit.encoder.layer.11.output.dense.weight\n",
      "vit.encoder.layer.11.output.dense.bias\n",
      "vit.encoder.layer.11.layernorm_before.weight\n",
      "vit.encoder.layer.11.layernorm_before.bias\n",
      "vit.encoder.layer.11.layernorm_after.weight\n",
      "vit.encoder.layer.11.layernorm_after.bias\n",
      "vit.layernorm.weight\n",
      "vit.layernorm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for i,p in model.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f223cf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.cls_token\n",
      "embeddings.position_embeddings\n",
      "embeddings.patch_embeddings.projection.weight\n",
      "embeddings.patch_embeddings.projection.bias\n",
      "encoder.layer.0.attention.attention.query.weight\n",
      "encoder.layer.0.attention.attention.query.bias\n",
      "encoder.layer.0.attention.attention.key.weight\n",
      "encoder.layer.0.attention.attention.key.bias\n",
      "encoder.layer.0.attention.attention.value.weight\n",
      "encoder.layer.0.attention.attention.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.layernorm_before.weight\n",
      "encoder.layer.0.layernorm_before.bias\n",
      "encoder.layer.0.layernorm_after.weight\n",
      "encoder.layer.0.layernorm_after.bias\n",
      "encoder.layer.1.attention.attention.query.weight\n",
      "encoder.layer.1.attention.attention.query.bias\n",
      "encoder.layer.1.attention.attention.key.weight\n",
      "encoder.layer.1.attention.attention.key.bias\n",
      "encoder.layer.1.attention.attention.value.weight\n",
      "encoder.layer.1.attention.attention.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.layernorm_before.weight\n",
      "encoder.layer.1.layernorm_before.bias\n",
      "encoder.layer.1.layernorm_after.weight\n",
      "encoder.layer.1.layernorm_after.bias\n",
      "encoder.layer.2.attention.attention.query.weight\n",
      "encoder.layer.2.attention.attention.query.bias\n",
      "encoder.layer.2.attention.attention.key.weight\n",
      "encoder.layer.2.attention.attention.key.bias\n",
      "encoder.layer.2.attention.attention.value.weight\n",
      "encoder.layer.2.attention.attention.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.layernorm_before.weight\n",
      "encoder.layer.2.layernorm_before.bias\n",
      "encoder.layer.2.layernorm_after.weight\n",
      "encoder.layer.2.layernorm_after.bias\n",
      "encoder.layer.3.attention.attention.query.weight\n",
      "encoder.layer.3.attention.attention.query.bias\n",
      "encoder.layer.3.attention.attention.key.weight\n",
      "encoder.layer.3.attention.attention.key.bias\n",
      "encoder.layer.3.attention.attention.value.weight\n",
      "encoder.layer.3.attention.attention.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.layernorm_before.weight\n",
      "encoder.layer.3.layernorm_before.bias\n",
      "encoder.layer.3.layernorm_after.weight\n",
      "encoder.layer.3.layernorm_after.bias\n",
      "encoder.layer.4.attention.attention.query.weight\n",
      "encoder.layer.4.attention.attention.query.bias\n",
      "encoder.layer.4.attention.attention.key.weight\n",
      "encoder.layer.4.attention.attention.key.bias\n",
      "encoder.layer.4.attention.attention.value.weight\n",
      "encoder.layer.4.attention.attention.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.layernorm_before.weight\n",
      "encoder.layer.4.layernorm_before.bias\n",
      "encoder.layer.4.layernorm_after.weight\n",
      "encoder.layer.4.layernorm_after.bias\n",
      "encoder.layer.5.attention.attention.query.weight\n",
      "encoder.layer.5.attention.attention.query.bias\n",
      "encoder.layer.5.attention.attention.key.weight\n",
      "encoder.layer.5.attention.attention.key.bias\n",
      "encoder.layer.5.attention.attention.value.weight\n",
      "encoder.layer.5.attention.attention.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.layernorm_before.weight\n",
      "encoder.layer.5.layernorm_before.bias\n",
      "encoder.layer.5.layernorm_after.weight\n",
      "encoder.layer.5.layernorm_after.bias\n",
      "encoder.layer.6.attention.attention.query.weight\n",
      "encoder.layer.6.attention.attention.query.bias\n",
      "encoder.layer.6.attention.attention.key.weight\n",
      "encoder.layer.6.attention.attention.key.bias\n",
      "encoder.layer.6.attention.attention.value.weight\n",
      "encoder.layer.6.attention.attention.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.layernorm_before.weight\n",
      "encoder.layer.6.layernorm_before.bias\n",
      "encoder.layer.6.layernorm_after.weight\n",
      "encoder.layer.6.layernorm_after.bias\n",
      "encoder.layer.7.attention.attention.query.weight\n",
      "encoder.layer.7.attention.attention.query.bias\n",
      "encoder.layer.7.attention.attention.key.weight\n",
      "encoder.layer.7.attention.attention.key.bias\n",
      "encoder.layer.7.attention.attention.value.weight\n",
      "encoder.layer.7.attention.attention.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.layernorm_before.weight\n",
      "encoder.layer.7.layernorm_before.bias\n",
      "encoder.layer.7.layernorm_after.weight\n",
      "encoder.layer.7.layernorm_after.bias\n",
      "encoder.layer.8.attention.attention.query.weight\n",
      "encoder.layer.8.attention.attention.query.bias\n",
      "encoder.layer.8.attention.attention.key.weight\n",
      "encoder.layer.8.attention.attention.key.bias\n",
      "encoder.layer.8.attention.attention.value.weight\n",
      "encoder.layer.8.attention.attention.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.layernorm_before.weight\n",
      "encoder.layer.8.layernorm_before.bias\n",
      "encoder.layer.8.layernorm_after.weight\n",
      "encoder.layer.8.layernorm_after.bias\n",
      "encoder.layer.9.attention.attention.query.weight\n",
      "encoder.layer.9.attention.attention.query.bias\n",
      "encoder.layer.9.attention.attention.key.weight\n",
      "encoder.layer.9.attention.attention.key.bias\n",
      "encoder.layer.9.attention.attention.value.weight\n",
      "encoder.layer.9.attention.attention.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.layernorm_before.weight\n",
      "encoder.layer.9.layernorm_before.bias\n",
      "encoder.layer.9.layernorm_after.weight\n",
      "encoder.layer.9.layernorm_after.bias\n",
      "encoder.layer.10.attention.attention.query.weight\n",
      "encoder.layer.10.attention.attention.query.bias\n",
      "encoder.layer.10.attention.attention.key.weight\n",
      "encoder.layer.10.attention.attention.key.bias\n",
      "encoder.layer.10.attention.attention.value.weight\n",
      "encoder.layer.10.attention.attention.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.layernorm_before.weight\n",
      "encoder.layer.10.layernorm_before.bias\n",
      "encoder.layer.10.layernorm_after.weight\n",
      "encoder.layer.10.layernorm_after.bias\n",
      "encoder.layer.11.attention.attention.query.weight\n",
      "encoder.layer.11.attention.attention.query.bias\n",
      "encoder.layer.11.attention.attention.key.weight\n",
      "encoder.layer.11.attention.attention.key.bias\n",
      "encoder.layer.11.attention.attention.value.weight\n",
      "encoder.layer.11.attention.attention.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.layernorm_before.weight\n",
      "encoder.layer.11.layernorm_before.bias\n",
      "encoder.layer.11.layernorm_after.weight\n",
      "encoder.layer.11.layernorm_after.bias\n",
      "layernorm.weight\n",
      "layernorm.bias\n"
     ]
    }
   ],
   "source": [
    "for i,p in model.vit.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "371aa826",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = model.vit.embeddings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20ca1f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vit.embeddings.cls_token.requires_grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb2fea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "1 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "2 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "3 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "4 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "5 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "6 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "7 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "8 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "9 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "10 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n",
      "11 ViTLayer(\n",
      "  (attention): ViTAttention(\n",
      "    (attention): ViTSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (output): ViTSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): ViTIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): ViTOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model.vit.encoder.layer):\n",
    "    print(i, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddfe914d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 50, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a418e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a81cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c543de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.vit.encoder(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98795057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vit.layernorm(a.last_hidden_state)[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "821ebe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier(model.vit.layernorm(a.last_hidden_state)[:,0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66e8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
